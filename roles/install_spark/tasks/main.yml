
# 解压 spark
- name: copy and unzip spark
  unarchive: 
    src: "{{ playbook_dir }}/packages/spark-2.4.7-bin-hadoop2.7.tar"
    dest: "{{ deploy_dir }}"


# 配置 slaves
- name: config slaves
  template: 
    src: "{{ playbook_dir }}/templates/spark/slaves.j2"
    dest: "{{ deploy_dir }}/spark-2.4.7-bin-hadoop2.7/conf/slaves"

# 配置 spark-env
- name: config spark-env
  template: 
    src: "{{ playbook_dir }}/templates/spark/spark-env.sh.j2"
    dest: "{{ deploy_dir }}/spark-2.4.7-bin-hadoop2.7/conf/spark-env.sh"


# 配置 spark-defaults
- name: config spark-defaults
  template:
      src: "{{ playbook_dir }}/templates/spark/spark-defaults.conf.j2"
      dest: "{{ deploy_dir }}/spark-2.4.7-bin-hadoop2.7/conf/spark-defaults.conf"

# 配置 log4j.properties
- name: config log4j.properties
  template:
    src: "{{ playbook_dir }}/templates/spark/log4j.properties.j2"
    dest: "{{ deploy_dir }}/spark-2.4.7-bin-hadoop2.7/conf/log4j.properties"


# 设置环境变量到bash_profile
- name: set spark env to bash_profile for no root user
  lineinfile: 
    dest: "/home/{{ ansible_ssh_user }}/.bash_profile"
    insertafter: "{{item.position}}" 
    line: "{{item.value}}" 
    state: present
  with_items:
    - {position: EOF, value: "export SPARK_HOME={{ deploy_dir }}/spark-2.4.7-bin-hadoop2.7/"}
    - {position: EOF, value: "export PATH=$SPARK_HOME/bin:$PATH"}
  when: ansible_ssh_user != "root"

# 刷新环境变量
- name: enforce env for no root user
  shell: "source /home/{{ ansible_ssh_user }}/.bash_profile"
  when: ansible_ssh_user != "root"
   
# 设置环境变量到bashrc
- name: set spark env to bashrc for no root user
  lineinfile: 
    dest: "/home/{{ ansible_ssh_user }}/.bashrc"
    insertafter: "{{item.position}}" 
    line: "{{item.value}}" 
    state: present
  with_items:
    - {position: EOF, value: "export SPARK_HOME={{ deploy_dir }}/spark-2.4.7-bin-hadoop2.7/"}
    - {position: EOF, value: "export PATH=$SPARK_HOME/bin:$PATH"}
  when: ansible_ssh_user != "root"

# 刷新环境变量
- name: enforce env for no root user
  shell: "source /home/{{ ansible_ssh_user }}/.bashrc"
  when: ansible_ssh_user != "root"

# 设置环境变量到 /etc/profile
- name: set spark env to /etc/profile for root user
  lineinfile:
    dest: "/etc/profile"
    insertafter: "{{item.position}}"
    line: "{{item.value}}"
    state: present
  with_items:
    - {position: EOF, value: "export SPARK_HOME={{ deploy_dir }}/spark-2.4.7-bin-hadoop2.7/"}
    - {position: EOF, value: "export PATH=$SPARK_HOME/bin:$PATH"}
  when: ansible_ssh_user == "root"

# 刷新环境变量
- name: enforce env for root user
  shell: "source /etc/profile"
  when: ansible_ssh_user == "root"

- name: change sbin file chmode
  file: 
    dest: "{{ deploy_dir }}/spark-2.4.7-bin-hadoop2.7/sbin"
    mode: 0755 
    recurse: yes
   
- name: change bin file chmode
  file: 
    dest: "{{ deploy_dir }}/spark-2.4.7-bin-hadoop2.7/bin"
    mode: 0755 
    recurse: yes


